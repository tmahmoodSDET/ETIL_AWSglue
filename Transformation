list of some common transformations you can perform using PySpark in AWS Glue:

Filter: Apply a condition to filter rows based on a specific criteria.
SelectFields: Select specific columns from the dataset.
RenameField: Rename a column in the dataset.
Map: Apply a function to each row in the dataset and return a new dataset.
Join: Combine two datasets based on a common key.
Distinct: Remove duplicate rows from the dataset.
Sort: Sort the dataset based on one or more columns.
GroupBy: Group the dataset by one or more columns and perform aggregation operations.
Aggregations: Perform aggregate functions like sum, count, average, etc. on columns in the dataset.
DropNullFields: Remove rows that contain null values in specific columns.
DropDuplicates: Remove rows that are duplicates across all columns.
SplitField: Split a column into multiple columns based on a delimiter.
MergeSchema: Combine multiple datasets with different schemas into a single dataset.
Repartition: Change the number of partitions in the dataset for improved performance.
Coalesce: Combine existing partitions into a smaller number of partitions.
WithColumn: Add a new column or update an existing column in the dataset.
Cast: Change the data type of a column.
UDF (User-Defined Function): Apply a custom function to transform data in a column.
Window Functions: Perform calculations over a window of rows, such as ranking, cumulative sum, etc.
Pivot: Pivot rows into columns based on a specific column's values.
These transformations provide a good starting point for building ETL pipelines with PySpark in AWS Glue.
